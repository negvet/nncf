{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NNCF-pytorch demo (tiny-imagenet/resnet-18)","provenance":[],"collapsed_sections":["K5HPrY_d-7cV","E01dMaR2_AFL","qMnYsGo9_MA8","L0tH9KdwtHhV"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"git68adWeq4l"},"source":["#Intro\r\n","\r\n","This notebook is based on 'ImageNet training in PyTorch' [example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\r\n","\r\n","The goal of this notebook is not to reach the best possible baselines for the implemented compression algorithms, but to demonstrate simple use cases of [NNCF](https://github.com/openvinotoolkit/nncf) with Pytorch. For more advanced usage refer to these [examples](https://github.com/openvinotoolkit/nncf/tree/develop/examples)\r\n","\r\n","To make downloading and training fast, we suggest to use resnet-18 model with tiny-imagenet dataset. But it is possible to change it.\r\n","\r\n","Demonstrated algorithms:\r\n","\r\n","- [Quantization](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md)\r\n","\r\n","- [Filter pruning](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Pruning.md)\r\n","\r\n","- [Sparsity](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Sparsity.md)"]},{"cell_type":"markdown","metadata":{"id":"6M1xndNu-z_2"},"source":["#Setup"]},{"cell_type":"code","metadata":{"id":"EIo5S145S0Ug"},"source":["!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install nncf==1.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwY9bxvth56S","executionInfo":{"status":"ok","timestamp":1612172037406,"user_tz":-180,"elapsed":265276,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["import os\n","import random\n","import shutil\n","import time\n","import warnings\n","import json\n","\n","import torch\n","import nncf  # Important - should be imported directly after torch\n","from nncf import create_compressed_model, NNCFConfig, register_default_init_args\n","\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.distributed as dist\n","import torch.optim\n","import torch.multiprocessing as mp\n","import torch.utils.data\n","import torch.utils.data.distributed\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.datasets"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fx4lCGIGNp28"},"source":["Tiny ImageNet dataset\n","* 100k images of shape 3x64x64\n","* 200 different classes: snakes, spaiders, cats, trucks, grasshopper, gull, etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gubHWwZELIRn","executionInfo":{"status":"ok","timestamp":1612172058919,"user_tz":-180,"elapsed":286773,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}},"outputId":"d085681c-cf77-48bb-c785-5f7821c82185"},"source":["from urllib.request import urlretrieve\n","\n","def download_tinyImg200(path,\n","                     url='http://cs231n.stanford.edu/tiny-imagenet-200.zip',\n","                     tarname='tiny-imagenet-200.zip'):\n","    if not os.path.exists(path):\n","        os.mkdir(path)\n","    urlretrieve(url, os.path.join(path,tarname))\n","    print (os.path.join(path,tarname))\n","    import zipfile\n","    zip_ref = zipfile.ZipFile(os.path.join(path,tarname), 'r')\n","    zip_ref.extractall()\n","    zip_ref.close()\n","\n","if not os.path.exists('tiny-imagenet-200'):\n","    download_tinyImg200('.')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["./tiny-imagenet-200.zip\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a4H9sI_uJOrT"},"source":["Connect to google drive to save and get access to the pretrained model on tiny-imagenet dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q58vQSLWIwBp","executionInfo":{"status":"ok","timestamp":1612173262755,"user_tz":-180,"elapsed":44516,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}},"outputId":"720f336e-1d46-4b7b-cc13-cdf87e6b0d7c"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eZX2GAh3W7ZT","executionInfo":{"status":"ok","timestamp":1612173341365,"user_tz":-180,"elapsed":761,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["# path to the saved model checkpoint\r\n","# can be any\r\n","PATH = '/content/drive/MyDrive/Colab Notebooks/nncf/'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5HPrY_d-7cV"},"source":["#Main function"]},{"cell_type":"code","metadata":{"id":"CLEwoilKiQYk","executionInfo":{"status":"ok","timestamp":1612173343119,"user_tz":-180,"elapsed":859,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def main(params):\n","\n","    arch = params['arch']\n","    num_classes = params['num_classes']\n","    init_lr = params['init_lr']\n","    # momentum = params['momentum']\n","    # weight_decay = params['weight_decay']\n","    batch_size = params['batch_size']\n","    workers = params['workers']\n","    pretrained = params['pretrained']\n","    resume = params['resume']\n","    checkpoint_compressed = params['checkpoint_compressed']\n","    data = params['data']\n","    evaluate = params['evaluate']\n","    start_epoch = params['start_epoch']\n","    epochs = params['epochs']\n","\n","    use_nncf = params['use_nncf']\n","    nncf_config_file = params['nncf_config_file']\n","\n","    best_acc1 = 0\n","\n","    # create model\n","    if pretrained:\n","        print(\"=> using pre-trained model '{}'\".format(arch))\n","        model = models.__dict__[arch](pretrained=True)\n","    else:\n","        print(\"=> creating model '{}'\".format(arch))\n","        model = models.__dict__[arch]()\n","    # update the last FC layer for tiny-imagenet number of classes\n","    model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n","\n","    if not torch.cuda.is_available():\n","        print('using CPU, this will be slow')\n","    else:\n","        print('using GPU')\n","        model.cuda()\n","        model.fc = model.fc.cuda()\n","\n","    # define loss function (criterion) and optimizer\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    # TODO: add the parameter in config files for optimizer type\n","    # optimizer = torch.optim.SGD(model.parameters(), init_lr,\n","    #                             momentum=momentum,\n","    #                             weight_decay=weight_decay)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n","\n","    # optionally resume from a checkpoint\n","    if resume:\n","        if os.path.isfile(resume):\n","            print(\"=> loading checkpoint '{}'\".format(resume))\n","            checkpoint = torch.load(resume)\n","\n","            start_epoch = checkpoint['epoch']\n","            print('resumed start_epoch', start_epoch)\n","            best_acc1 = checkpoint['best_acc1']\n","\n","            model.load_state_dict(checkpoint['state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer'])\n","            print(\"=> loaded checkpoint '{}' (epoch {}, best_acc1 {:6.2f})\"\n","                  .format(resume, checkpoint['epoch'], checkpoint['best_acc1']))\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(resume))\n","\n","    # Data loading code\n","    traindir = os.path.join(data, 'train')\n","    valdir = os.path.join(data, 'val')\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","    dataset = datasets.ImageFolder(\n","        traindir,\n","        transforms.Compose([\n","            transforms.Resize(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize,\n","        ]))\n","    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True,\n","        num_workers=workers, pin_memory=True, sampler=None)\n","\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset, batch_size=batch_size, shuffle=False,\n","        num_workers=workers, pin_memory=True)\n","\n","    if use_nncf:\n","        # Load a configuration file to specify compression\n","        nncf_config = NNCFConfig.from_json(nncf_config_file)\n","        # Provide data loaders for compression algorithm initialization, if necessary\n","        nncf_config = register_default_init_args(nncf_config, train_loader, criterion)\n","        # Apply the specified compression algorithms to the model\n","        print('=> compressing the model with {}'.format(nncf_config_file))\n","        compression_ctrl, model = create_compressed_model(model, nncf_config)\n","\n","    if evaluate:\n","        validate(val_loader, model, criterion)\n","        return\n","\n","    for epoch in range(start_epoch, epochs):\n","        adjust_learning_rate(optimizer, epoch, init_lr)\n","\n","        if use_nncf:\n","            # update compression scheduler state at the begin of the epoch\n","            compression_ctrl.scheduler.epoch_step()\n","            # train for one epoch with nncf\n","            train(train_loader, model, criterion, optimizer, epoch, use_nncf, compression_ctrl=compression_ctrl)\n","        else:\n","            # train for one epoch without nncf\n","            train(train_loader, model, criterion, optimizer, epoch, use_nncf, compression_ctrl=None)\n","\n","        # evaluate on validation set\n","        acc1 = validate(val_loader, model, criterion)\n","\n","        # remember best acc@1 and save checkpoint\n","        is_best = acc1 > best_acc1\n","        best_acc1 = max(acc1, best_acc1)\n","\n","        if not use_nncf:\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'arch': arch,\n","                'state_dict': model.state_dict(),\n","                'best_acc1': best_acc1,\n","                'optimizer' : optimizer.state_dict(),\n","            }, is_best)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E01dMaR2_AFL"},"source":["#Train function"]},{"cell_type":"code","metadata":{"id":"940rcAIyiXml","executionInfo":{"status":"ok","timestamp":1612173343889,"user_tz":-180,"elapsed":705,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def train(train_loader, model, criterion, optimizer, epoch, use_nncf, compression_ctrl):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    data_time = AverageMeter('Data', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(\n","        len(train_loader),\n","        [batch_time, data_time, losses, top1, top5],\n","        prefix=\"Epoch: [{}]\".format(epoch))\n","\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (images, target) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        if use_nncf:\n","            compression_ctrl.scheduler.step()\n","\n","        if torch.cuda.is_available():\n","            images = images.cuda()\n","            target = target.cuda()\n","\n","        # compute output\n","        output = model(images)\n","        loss = criterion(output, target)\n","\n","        if use_nncf:\n","            compression_loss = compression_ctrl.loss()\n","            loss += compression_loss\n","\n","        # measure accuracy and record loss\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), images.size(0))\n","        top1.update(acc1[0], images.size(0))\n","        top5.update(acc5[0], images.size(0))\n","\n","        # compute gradient and do opt step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        print_frequency = 10\n","        if i % print_frequency == 0:\n","            progress.display(i)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CoNr8qwm_El2"},"source":["#Validate function"]},{"cell_type":"code","metadata":{"id":"KgnugrWgicWC","executionInfo":{"status":"ok","timestamp":1612173345501,"user_tz":-180,"elapsed":804,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def validate(val_loader, model, criterion):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(\n","        len(val_loader),\n","        [batch_time, losses, top1, top5],\n","        prefix='Test: ')\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        end = time.time()\n","        for i, (images, target) in enumerate(val_loader):\n","            if torch.cuda.is_available():\n","                images = images.cuda()\n","                target = target.cuda()\n","\n","            # compute output\n","            output = model(images)\n","            loss = criterion(output, target)\n","\n","            # measure accuracy and record loss\n","            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","            losses.update(loss.item(), images.size(0))\n","            top1.update(acc1[0], images.size(0))\n","            top5.update(acc5[0], images.size(0))\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            print_frequency = 10\n","            if i % print_frequency == 0:\n","                progress.display(i)\n","\n","        # TODO: this should also be done with the ProgressMeter\n","        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n","              .format(top1=top1, top5=top5))\n","\n","    return top1.avg"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMnYsGo9_MA8"},"source":["#Helpers"]},{"cell_type":"code","metadata":{"id":"R724tbxcidQE","executionInfo":{"status":"ok","timestamp":1612173345821,"user_tz":-180,"elapsed":502,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n","    torch.save(state, os.path.join(PATH, filename))\n","    if is_best:\n","        shutil.copyfile(os.path.join(PATH, filename), os.path.join(PATH, 'model_best.pth.tar'))\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter(object):\n","    def __init__(self, num_batches, meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def display(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print('\\t'.join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = '{:' + str(num_digits) + 'd}'\n","        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcSjyLBwiqBx","executionInfo":{"status":"ok","timestamp":1612173346155,"user_tz":-180,"elapsed":833,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def adjust_learning_rate(optimizer, epoch, init_lr):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    lr = init_lr * (0.1 ** (epoch // 30))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"118rlV22_PB5"},"source":["#Main\r\n","\r\n","Pipeline:\r\n","\r\n","- Train without NNCF (for example 10 epochs with lr=1e-3 and then 5 epochs with lr=1e-4 for resnet-18), save best checkpoint in float precision\r\n","\r\n","- Load best float checkpoint and compress it with selected algorithm (convert the model to NNCFNetwork format by enabling NNCF) TODO: be able to load the compressed model for further tuning\r\n","\r\n","- Tune the compressed model (train with enabled NNCF, define the tune parameters in the corresponding configuraion files)\r\n","\r\n","- Save compressed tuned model (or convert to ONNX)"]},{"cell_type":"markdown","metadata":{"id":"L0tH9KdwtHhV"},"source":["## Configuration files for compression algorithms"]},{"cell_type":"code","metadata":{"id":"kaf3cjgSyQh5","executionInfo":{"status":"ok","timestamp":1612173351445,"user_tz":-180,"elapsed":989,"user":{"displayName":"Evgeny Tsykunov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-PAF33_jUCD6ZuQFo__i7FR2S1P3MW5maTi34X2o=s64","userId":"06101548677895852703"}}},"source":["def create_json_files(batch_size, input_size):\r\n","    \"\"\"\r\n","    Define configurations for compression algorithms\r\n","    Create the json files\r\n","    Return the configurations as dictinary objects\r\n","    \"\"\"\r\n","\r\n","    config_dir = 'config_files'\r\n","    if not os.path.exists(config_dir):\r\n","        os.makedirs(config_dir)\r\n","\r\n","    def write_json(json_obj, json_name):\r\n","        with open(os.path.join(config_dir, json_name), 'w') as jsonFile:\r\n","            json.dump(json_obj, jsonFile)\r\n","\r\n","    # Define config objects below\r\n","    configs = {}\r\n","\r\n","    # Quantization int8\r\n","    # https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md\r\n","    configs['quantization.json'] = {\r\n","\r\n","            \"input_info\": {\r\n","              \"sample_size\": [batch_size, 3, input_size, input_size]\r\n","            },\r\n","\r\n","            \"epochs\": 1, # number of epochs to tune\r\n","\r\n","            \"optimizer\": {\r\n","                \"base_lr\": 1e-5 # learning rate for the optimizer during tuning\r\n","            },\r\n","\r\n","            \"compression\": {\r\n","                    \"algorithm\": \"quantization\", # specify the algorithm here\r\n","            }\r\n","    }\r\n","\r\n","    # Filter pruning\r\n","    # https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Pruning.md\r\n","    configs['pruning.json'] = {\r\n","        \r\n","            \"input_info\": {\r\n","              \"sample_size\": [batch_size, 3, input_size, input_size]\r\n","            },\r\n","\r\n","            \"epochs\": 1, # number of epochs to tune\r\n","\r\n","            \"optimizer\": {\r\n","                \"base_lr\": 1e-3 # learning rate for the optimizer during tuning\r\n","            },\r\n","\r\n","            \"compression\": {\r\n","                \"algorithm\": \"filter_pruning\",\r\n","            }\r\n","    }\r\n","\r\n","    # Sparsity\r\n","    # https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Sparsity.md\r\n","    configs['sparsity.json'] = {\r\n","        \r\n","            \"input_info\": {\r\n","              \"sample_size\": [batch_size, 3, input_size, input_size]\r\n","            },\r\n","\r\n","            \"epochs\": 50, # number of epochs to tune\r\n","\r\n","            \"optimizer\": {\r\n","                \"base_lr\": 1e-3 # learning rate for the optimizer during tuning\r\n","            },\r\n","\r\n","            \"compression\": {\r\n","                \"algorithm\": \"magnitude_sparsity\",\r\n","                \"sparsity_init\": 0.1, # Initial value of the sparsity level applied to the model in 'create_compressed_model' function\r\n","                \"params\": {\r\n","                    \"schedule\": \"multistep\", # The type of scheduling to use for adjusting the target sparsity level                   \r\n","                    \"multistep_steps\": [ # A list of scheduler steps at which to transition to the next scheduled sparsity level (multistep scheduler only).\r\n","                        5,\r\n","                        10,\r\n","                        20,\r\n","                        30,\r\n","                        40\r\n","                    ],\r\n","                    \"multistep_sparsity_levels\": [ # Levels of sparsity to use at each step of the scheduler as specified in the 'multistep_steps' attribute. The first sparsity level will be applied immediately, so the length of this list should be larger than the length of the 'steps' by one.\"\r\n","                        0.1,\r\n","                        0.2,\r\n","                        0.3,\r\n","                        0.4,\r\n","                        0.5,\r\n","                        0.6\r\n","                    ],\r\n","                }\r\n","            }\r\n","    }\r\n","\r\n","    # create json files, that will be used by nncf later\r\n","    for config_key, config_val in configs.items():\r\n","        write_json(config_val, config_key)\r\n","    \r\n","    return configs"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9d8LOmKut36x"},"source":["## Train/tune"]},{"cell_type":"code","metadata":{"id":"qk0Nub3KiqZK"},"source":["params = {}\n","\n","params['arch'] = 'resnet18'\n","params['num_classes'] = 200 # 200 is for tiny-imagenet, default is 1000 for imagenet\n","params['data'] = 'tiny-imagenet-200' # path to data\n","\n","params['init_lr'] = 1e-4\n","params['batch_size'] = 256\n","params['workers'] = 4\n","params['start_epoch'] = 0 # updated automatically if training is resumed\n","params['epochs'] = 15 # 15 is for full precision training, will be updated (increased) in case of tuning with nncf \n","params['pretrained'] = True # pretrained model on Imagenet\n","\n","params['resume'] = PATH + 'model_best.pth.tar'  # path to latest checkpoint (or None)\n","params['checkpoint_compressed'] = False\n","\n","params['evaluate'] = False\n","\n","params['use_nncf'] = True\n","\n","if params['use_nncf']:\n","    # create all config files\n","    configs = create_json_files(params['batch_size'], input_size=224)\n","\n","    # choose config file\n","    algorithm_config = 'quantization.json'\n","    params['nncf_config_file'] = 'config_files/' + algorithm_config\n","\n","    # update tune params to fit certain compression algorithm\n","    params['init_lr'] = configs[algorithm_config]['optimizer']['base_lr']\n","    params['epochs'] = params['epochs'] + configs[algorithm_config]['epochs']\n","\n","\n","# Run certain algorithm once\n","main(params)\n","\n","# Iterate over algorithms\n","algorithm_configs = ['quantization.json', 'pruning.json', 'sparsity.json']\n","for algorithm_config in algorithm_configs:\n","    # Run the tuning procedure:\n","    # print(algorithm_config)\n","    # params['nncf_config_file'] = 'config_files/' + algorithm_config\n","    # update tune params\n","    # main(params)\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENAbqFpdWSlE"},"source":["# Export to ONNX that is supported by the OpenVINO™ toolkit\r\n","compression_ctrl.export_model(\"model_compressed.onnx\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_I_G-g9TPWkl"},"source":["#Export to OpenVINO™ Intermediate Representation (IR)\r\n","To export a model to the OpenVINO IR and run it using the Intel® Deep Learning Deployment Toolkit, refer to this [tutorial](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html)."]}]}